{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwUFGOOaLcoH"
      },
      "source": [
        "# Model training notebook for **EfficientSpeech: An On-Device Text to Speech Model**\n",
        "## *This notebook requires a GPU for training.*  \n",
        "\n",
        "  This goal of this notebook is to demonstrate training new checkpoints for EfficientSpeech.\n",
        "\n",
        "  The dataset format used for training is FastSpeech2. Please use preprocess_dataset notebook for dataset preparation.\n",
        "  \n",
        "\n",
        "#### Links\n",
        "Official EfficientSpeech repository: https://github.com/roatienza/efficientspeech  \n",
        "Paper: https://ieeexplore.ieee.org/abstract/document/10094639\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#  \n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "UFDrLqlzTDT3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNmrrM_ylkrW"
      },
      "source": [
        "# Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z60gbCONlepj",
        "outputId": "95458f68-92d5-4c8d-f5e7-44b5317bb763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#  \n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DLd56i_FTHFd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liZc2svqHrM0"
      },
      "source": [
        "# Configuration options\n",
        "#### Make sure to configure the settings in the `Configuration Settings` section below before running further cells.\n",
        "\n",
        "##### Dataset\n",
        "* dataset_name: name of the dataset (default 'MyDataset')\n",
        "* dataset_location: absolute path to the prepared dataset folder (default: '/content/output_dataset')\n",
        "* speaker_name: name of the speaker in raw_data folder (default: 'universal')\n",
        "* config_dir: absolute path to EfficientSpeech configuration directory\n",
        "* lexicon_path: absolute path to a .txt file with the lexicon/dictionary the dataset is prepared for (defaults to `librispeech-lexicon.txt`)\n",
        "\n",
        "##### Output\n",
        "* output_dir: A path to save all generated .ckpt files + logs to. A folder with your dataset name will be created in this folder.\n",
        "* infer_device: Device used for inference after training. One of 'cuda', 'cpu' (default: 'cuda')\n",
        "\n",
        "##### Model training options\n",
        "* accelerator: One of `cpu`, `gpu`\n",
        "* devices: Will be mapped to either `gpus`, `tpu_cores`, `num_processes` or `ipus`, based on the accelerator type, per pytorch-lightning documentation.\n",
        "* batch_size: (default: 128)\n",
        "* num_workers: (default: 4)\n",
        "* precision: (default: 16-mixed)\n",
        "* model_size_to_train: One of 'tiny', 'small', 'base' (default: tiny)\n",
        "* max_epochs: The number of epochs to stop training at (default: 5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IJwbFktvjXL",
        "outputId": "fd952f21-fcbb-4ba2-dc2b-5ab3691a678f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Command line arguments: --accelerator gpu --devices 1 --num_workers 4 --precision 16-mixed --batch-size 128 --head 1 --reduction 4 --expansion 1 --kernel-size 3 --n-blocks 2 --block-depth 2 --max_epochs 5000 --infer-device cuda\n"
          ]
        }
      ],
      "source": [
        "# Dataset parameters\n",
        "dataset_name = 'MyDataset' #@param {type:\"string\"}\n",
        "dataset_location = '/content/output_dataset' #@param {type:\"string\"}\n",
        "speaker_name = 'universal' #@param {type:'string'}\n",
        "\n",
        "config_dir = '/content/efficientspeech/config' #@param {type:\"string\"}\n",
        "output_dir = '/content/drive/MyDrive/saved_checkpoints' #@param {type:\"string\"}\n",
        "lexicon_path = '/content/efficientspeech/lexicon/librispeech-lexicon.txt' #@param {type:\"string\"}\n",
        "\n",
        "# Model training Options\n",
        "cmd_line_opts = ''\n",
        "\n",
        "# Accelerator is TPU for Colab\n",
        "accelerator = 'gpu' #@param {type:'string'} ['gpu', 'cpu']\n",
        "cmd_line_opts += f' --accelerator {accelerator}'\n",
        "\n",
        "# Devices\n",
        "devices = 1 #@param {type:'integer'}\n",
        "cmd_line_opts += f' --devices {devices}'\n",
        "\n",
        "# Num Workers\n",
        "num_workers = 4 #@param {type:'integer'}\n",
        "cmd_line_opts += f' --num_workers {num_workers}'\n",
        "\n",
        "# Precision\n",
        "precision = '16-mixed' #@param {type:'string'} [\"bf16-mixed\", \"16-mixed\", \"16\", \"32\", \"64\"]\n",
        "cmd_line_opts += f' --precision {precision}'\n",
        "\n",
        "# Batch size (128 is default)\n",
        "batch_size = 128 #@param [16, 32, 64, 128]\n",
        "cmd_line_opts += f' --batch-size {batch_size}'\n",
        "\n",
        "# Cmd line opts for training different size models \n",
        "# Specify options explicitly for tiny to address this error\n",
        "# RuntimeError: Calculated padded input size per channel: (4). Kernel size: (5). Kernel size can't be greater than actual input size \n",
        "model_size_to_train = \"tiny\" #@param [\"tiny\", \"small\", \"base\"]\n",
        "match (model_size_to_train):\n",
        "  case \"small\":\n",
        "    model_opts = ' --head 1 --reduction 1 --expansion 2  --kernel-size 5 --n-blocks 3 --block-depth 3'\n",
        "  case \"base\":\n",
        "    model_opts = ' --head 2 --reduction 1 --expansion 2  --kernel-size 5 --n-blocks 3 --block-depth 3'\n",
        "  case _: #tiny\n",
        "    model_opts = ' --head 1 --reduction 4 --expansion 1 --kernel-size 3 --n-blocks 2 --block-depth 2'\n",
        "cmd_line_opts += model_opts\n",
        "\n",
        "# Max epochs\n",
        "max_epochs = 5000 #@param {type:\"integer\"}\n",
        "cmd_line_opts += f' --max_epochs {max_epochs}'\n",
        "\n",
        "# Inference device\n",
        "infer_device = 'cuda' #@param {type:\"string\"} [\"cuda\", \"cpu\"]\n",
        "cmd_line_opts += f' --infer-device {infer_device}'\n",
        "\n",
        "!echo Command line arguments: $cmd_line_opts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#  \n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "72xML0rwYY-Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfVl2VpJ1SZm"
      },
      "source": [
        "# Unzip Preprocessed dataset\n",
        "If you used the prepare_dataset notebook to prepare your dataset, this will extract it to the default location (`/content/output_dataset`).\n",
        "After extraction, the directory structure will look like this:\n",
        "* `/content/output_dataset`\n",
        "  - `configs`\n",
        "  - `raw_data`\n",
        "  - `preprocessed_data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XU-qGZ31SZm",
        "outputId": "645cb96f-4dbc-4043-ef0d-8311f22aada9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Archive:  /content/drive/MyDrive/output_dataset/MyDataset.zip\n",
            "   creating: /content/output_dataset/\n",
            "   creating: /content/output_dataset/raw_data/\n",
            "   creating: /content/output_dataset/raw_data/universal/\n",
            "  inflating: /content/output_dataset/raw_data/universal/p303_008.wav  \n",
            "  inflating: /content/output_dataset/raw_data/universal/p303_004.lab  \n",
            "  inflating: /content/output_dataset/raw_data/universal/p303_003.wav  \n",
            "  inflating: /content/output_dataset/raw_data/universal/p303_005.wav  \n",
            "  inflating: /content/output_dataset/raw_data/universal/p303_005.lab  \n",
            "  inflating: /content/output_dataset/raw_data/universal/p303_001.wav  \n",
            "  inflating: /content/output_dataset/raw_data/universal/p303_003.lab  \n",
            "  inflating: /content/output_dataset/raw_data/universal/p303_004.wav  \n",
            "  inflating: /content/output_dataset/raw_data/universal/metadata.csv  \n",
            "  inflating: /content/output_dataset/raw_data/universal/p303_007.wav  \n",
            "  inflating: /content/output_dataset/raw_data/universal/p303_007.lab  \n",
            "  inflating: /content/output_dataset/raw_data/universal/p303_009.lab  \n",
            "  inflating: /content/output_dataset/raw_data/universal/p303_009.wav  \n",
            "  inflating: /content/output_dataset/raw_data/universal/p303_008.lab  \n",
            "  inflating: /content/output_dataset/raw_data/universal/p303_006.wav  \n",
            "  inflating: /content/output_dataset/raw_data/universal/p303_006.lab  \n",
            "  inflating: /content/output_dataset/raw_data/universal/p303_002.wav  \n",
            "  inflating: /content/output_dataset/raw_data/universal/p303_002.lab  \n",
            " extracting: /content/output_dataset/raw_data/universal/p303_001.lab  \n",
            "   creating: /content/output_dataset/configs/\n",
            "   creating: /content/output_dataset/configs/MyDataset/\n",
            "  inflating: /content/output_dataset/configs/MyDataset/model.yaml  \n",
            "  inflating: /content/output_dataset/configs/MyDataset/train.yaml  \n",
            "  inflating: /content/output_dataset/configs/MyDataset/preprocess.yaml  \n",
            "   creating: /content/output_dataset/preprocessed_data/\n",
            "   creating: /content/output_dataset/preprocessed_data/MyDataset/\n",
            "   creating: /content/output_dataset/preprocessed_data/MyDataset/mel/\n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/mel/universal-mel-p303_005.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/mel/universal-mel-p303_003.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/mel/universal-mel-p303_007.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/mel/universal-mel-p303_001.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/mel/universal-mel-p303_006.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/mel/universal-mel-p303_002.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/mel/universal-mel-p303_004.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/mel/universal-mel-p303_009.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/mel/universal-mel-p303_008.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/train.txt  \n",
            "   creating: /content/output_dataset/preprocessed_data/MyDataset/energy/\n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/energy/universal-energy-p303_009.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/energy/universal-energy-p303_003.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/energy/universal-energy-p303_004.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/energy/universal-energy-p303_001.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/energy/universal-energy-p303_007.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/energy/universal-energy-p303_002.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/energy/universal-energy-p303_005.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/energy/universal-energy-p303_006.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/energy/universal-energy-p303_008.npy  \n",
            "   creating: /content/output_dataset/preprocessed_data/MyDataset/TextGrid/\n",
            "   creating: /content/output_dataset/preprocessed_data/MyDataset/TextGrid/universal/\n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/TextGrid/universal/p303_005.TextGrid  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/TextGrid/universal/p303_001.TextGrid  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/TextGrid/universal/p303_008.TextGrid  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/TextGrid/universal/p303_002.TextGrid  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/TextGrid/universal/p303_003.TextGrid  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/TextGrid/universal/p303_007.TextGrid  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/TextGrid/universal/p303_004.TextGrid  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/TextGrid/universal/p303_006.TextGrid  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/TextGrid/universal/p303_009.TextGrid  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/stats.json  \n",
            "   creating: /content/output_dataset/preprocessed_data/MyDataset/pitch/\n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/pitch/universal-pitch-p303_004.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/pitch/universal-pitch-p303_008.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/pitch/universal-pitch-p303_002.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/pitch/universal-pitch-p303_007.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/pitch/universal-pitch-p303_006.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/pitch/universal-pitch-p303_003.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/pitch/universal-pitch-p303_001.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/pitch/universal-pitch-p303_009.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/pitch/universal-pitch-p303_005.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/val.txt  \n",
            "   creating: /content/output_dataset/preprocessed_data/MyDataset/duration/\n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/duration/universal-duration-p303_008.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/duration/universal-duration-p303_004.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/duration/universal-duration-p303_007.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/duration/universal-duration-p303_006.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/duration/universal-duration-p303_005.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/duration/universal-duration-p303_003.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/duration/universal-duration-p303_009.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/duration/universal-duration-p303_002.npy  \n",
            "  inflating: /content/output_dataset/preprocessed_data/MyDataset/duration/universal-duration-p303_001.npy  \n",
            " extracting: /content/output_dataset/preprocessed_data/MyDataset/speakers.json  \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "%cd /content/\n",
        "zip_file_location = '/content/drive/MyDrive/output_dataset/MyDataset.zip' #@param\n",
        "!unzip -u $zip_file_location -d /\n",
        "\n",
        "# Sanity check - Check if folders are named what we expect\n",
        "expected_folders = [os.path.join(dataset_location, 'preprocessed_data', dataset_name),\n",
        "                  os.path.join(dataset_location, 'raw_data', speaker_name)]\n",
        "for folder_name in expected_folders:\n",
        "  assert os.path.exists(folder_name), f'No folder named {folder_name} exists, please check directory structure is correct and folder exists'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CSrbDeIloq3"
      },
      "source": [
        "# Setup dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX4lXGtlhkQ9",
        "outputId": "5849f7fe-5504-4d1e-a278-959145481474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientspeech'...\n",
            "remote: Enumerating objects: 1511, done.\u001b[K\n",
            "remote: Counting objects: 100% (162/162), done.\u001b[K\n",
            "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
            "remote: Total 1511 (delta 87), reused 122 (delta 58), pack-reused 1349\u001b[K\n",
            "Receiving objects: 100% (1511/1511), 5.03 MiB | 21.66 MiB/s, done.\n",
            "Resolving deltas: 100% (983/983), done.\n",
            "2023-06-01 02:37:07 URL:https://objects.githubusercontent.com/github-production-release-asset-2e65be/483135884/d61e6948-debe-4924-ad39-32ba0f29a53b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230601%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230601T023706Z&X-Amz-Expires=300&X-Amz-Signature=cdbc78d3e885d2989ac7e2a11fb8bdecba35df996ca7b2663f90aa7a298d2f0d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=483135884&response-content-disposition=attachment%3B%20filename%3Dbase_eng_4M.ckpt&response-content-type=application%2Foctet-stream [51366419/51366419] -> \"/content/efficientspeech/checkpoints/base_eng_4M.ckpt\" [1]\n",
            "2023-06-01 02:37:08 URL:https://objects.githubusercontent.com/github-production-release-asset-2e65be/483135884/0b331078-dc6c-4435-8f44-4609ff3295ba?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230601%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230601T023707Z&X-Amz-Expires=300&X-Amz-Signature=0c6defcdb700ce0f698fe20199bad89f8a930a2048b01564889ef82b8fac422b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=483135884&response-content-disposition=attachment%3B%20filename%3Dsmall_eng_952k.ckpt&response-content-type=application%2Foctet-stream [15334819/15334819] -> \"/content/efficientspeech/checkpoints/small_eng_952k.ckpt\" [1]\n",
            "2023-06-01 02:37:08 URL:https://objects.githubusercontent.com/github-production-release-asset-2e65be/483135884/f521ed8e-9ff9-4b90-9c24-5b77bc02f6d1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230601%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230601T023708Z&X-Amz-Expires=300&X-Amz-Signature=f48f559bf193143f4ade23309e451f60c61cf3e3b4af8d8ddbdc229575ee25e4&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=483135884&response-content-disposition=attachment%3B%20filename%3Dtiny_eng_266k.ckpt&response-content-type=application%2Foctet-stream [7086163/7086163] -> \"/content/efficientspeech/checkpoints/tiny_eng_266k.ckpt\" [1]\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "\n",
        "# Delete existing\n",
        "## !rm -rf /content/efficientspeech\n",
        "\n",
        "# Clone repository (Note: this is my fork with additional training options)\n",
        "!git clone https://github.com/roatienza/efficientspeech\n",
        "\n",
        "# Make training config directory\n",
        "dataset_config_dir = os.path.join(config_dir, dataset_name)\n",
        "!mkdir $dataset_config_dir\n",
        "\n",
        "# Download model files\n",
        "!mkdir /content/efficientspeech/checkpoints\n",
        "!wget --continue -nv -O /content/efficientspeech/checkpoints/base_eng_4M.ckpt  https://github.com/roatienza/efficientspeech/releases/download/pytorch2.0.1/base_eng_4M.ckpt \n",
        "!wget --continue -nv -O /content/efficientspeech/checkpoints/small_eng_952k.ckpt  https://github.com/roatienza/efficientspeech/releases/download/pytorch2.0.1/small_eng_952k.ckpt\n",
        "!wget --continue -nv -O /content/efficientspeech/checkpoints/tiny_eng_266k.ckpt  https://github.com/roatienza/efficientspeech/releases/download/pytorch2.0.1/tiny_eng_266k.ckpt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "haFIWkj6ls0p",
        "outputId": "b83184aa-2bdf-425e-eeef-0124f4580996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.12.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.4.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.22.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.40.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/efficientspeech/requirements.txt (line 1)) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/efficientspeech/requirements.txt (line 2)) (0.15.2+cu118)\n",
            "Collecting lightning>=2.0.2 (from -r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading lightning-2.0.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics>=0.11.4 (from -r /content/efficientspeech/requirements.txt (line 4))\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unidecode (from -r /content/efficientspeech/requirements.txt (line 5))\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from -r /content/efficientspeech/requirements.txt (line 6)) (6.0.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r /content/efficientspeech/requirements.txt (line 7)) (3.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r /content/efficientspeech/requirements.txt (line 8)) (1.10.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from -r /content/efficientspeech/requirements.txt (line 9)) (8.4.0)\n",
            "Collecting einops (from -r /content/efficientspeech/requirements.txt (line 10))\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting g2p-en (from -r /content/efficientspeech/requirements.txt (line 11))\n",
            "  Downloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting validators (from -r /content/efficientspeech/requirements.txt (line 12))\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting onnx (from -r /content/efficientspeech/requirements.txt (line 13))\n",
            "  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime (from -r /content/efficientspeech/requirements.txt (line 14))\n",
            "  Downloading onnxruntime-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf==3.20.2 (from -r /content/efficientspeech/requirements.txt (line 15))\n",
            "  Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.23.5 (from -r /content/efficientspeech/requirements.txt (line 16))\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from -r /content/efficientspeech/requirements.txt (line 18)) (0.10.0.post2)\n",
            "Collecting tgt (from -r /content/efficientspeech/requirements.txt (line 21))\n",
            "  Downloading tgt-1.4.4.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyworld (from -r /content/efficientspeech/requirements.txt (line 22))\n",
            "  Downloading pyworld-0.3.3.tar.gz (218 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.7/218.7 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sounddevice (from -r /content/efficientspeech/requirements.txt (line 24))\n",
            "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r /content/efficientspeech/requirements.txt (line 1)) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r /content/efficientspeech/requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r /content/efficientspeech/requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r /content/efficientspeech/requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r /content/efficientspeech/requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r /content/efficientspeech/requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=2.0.1->-r /content/efficientspeech/requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=2.0.1->-r /content/efficientspeech/requirements.txt (line 1)) (16.0.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.15.2->-r /content/efficientspeech/requirements.txt (line 2)) (2.27.1)\n",
            "Requirement already satisfied: PyYAML<8.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (6.0)\n",
            "Collecting arrow<3.0,>=1.2.0 (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (4.11.2)\n",
            "Requirement already satisfied: click<10.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (8.1.3)\n",
            "Collecting croniter<1.4.0,>=1.3.0 (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading croniter-1.3.15-py2.py3-none-any.whl (19 kB)\n",
            "Collecting dateutils<2.0 (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading dateutils-0.6.12-py2.py3-none-any.whl (5.7 kB)\n",
            "Collecting deepdiff<8.0,>=5.7.0 (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading deepdiff-6.3.0-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi<0.89.0,>=0.69.0 (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading fastapi-0.88.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec<2024.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (2023.4.0)\n",
            "Collecting inquirer<5.0,>=2.10.0 (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading inquirer-3.1.3-py3-none-any.whl (18 kB)\n",
            "Collecting lightning-cloud>=0.5.34 (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading lightning_cloud-0.5.36-py3-none-any.whl (562 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m562.4/562.4 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities<2.0,>=0.7.0 (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (23.1)\n",
            "Requirement already satisfied: psutil<7.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (5.9.5)\n",
            "Requirement already satisfied: pydantic<4.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (1.10.7)\n",
            "Requirement already satisfied: rich<15.0,>=12.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (13.3.4)\n",
            "Collecting starlette (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starsessions<2.0,>=1.2.1 (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading starsessions-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (4.65.0)\n",
            "Requirement already satisfied: traitlets<7.0,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (5.7.1)\n",
            "Requirement already satisfied: urllib3<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (1.26.15)\n",
            "Collecting uvicorn<2.0 (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (1.5.1)\n",
            "Collecting websockets<12.0 (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading pytorch_lightning-2.0.2-py3-none-any.whl (719 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.0/719.0 kB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/efficientspeech/requirements.txt (line 7)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/efficientspeech/requirements.txt (line 7)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/efficientspeech/requirements.txt (line 7)) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/efficientspeech/requirements.txt (line 7)) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/efficientspeech/requirements.txt (line 7)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/efficientspeech/requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.10/dist-packages (from g2p-en->-r /content/efficientspeech/requirements.txt (line 11)) (3.8.1)\n",
            "Collecting distance>=0.1.3 (from g2p-en->-r /content/efficientspeech/requirements.txt (line 11))\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from validators->-r /content/efficientspeech/requirements.txt (line 12)) (4.4.2)\n",
            "Collecting coloredlogs (from onnxruntime->-r /content/efficientspeech/requirements.txt (line 14))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime->-r /content/efficientspeech/requirements.txt (line 14)) (23.3.3)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/efficientspeech/requirements.txt (line 18)) (3.0.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/efficientspeech/requirements.txt (line 18)) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/efficientspeech/requirements.txt (line 18)) (1.2.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/efficientspeech/requirements.txt (line 18)) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/efficientspeech/requirements.txt (line 18)) (0.12.1)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/efficientspeech/requirements.txt (line 18)) (1.6.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/efficientspeech/requirements.txt (line 18)) (0.3.5)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/efficientspeech/requirements.txt (line 18)) (0.2)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/efficientspeech/requirements.txt (line 18)) (1.0.5)\n",
            "Requirement already satisfied: cython>=0.24 in /usr/local/lib/python3.10/dist-packages (from pyworld->-r /content/efficientspeech/requirements.txt (line 22)) (0.29.34)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice->-r /content/efficientspeech/requirements.txt (line 24)) (1.15.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<6.0,>=4.8.0->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (2.4.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice->-r /content/efficientspeech/requirements.txt (line 24)) (2.21)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateutils<2.0->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (2022.7.1)\n",
            "Collecting ordered-set<4.2.0,>=4.0.2 (from deepdiff<8.0,>=5.7.0->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Collecting starlette (from lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading starlette-0.22.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (3.6.2)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec<2024.0,>=2022.5.0->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blessed>=1.19.0 (from inquirer<5.0,>=2.10.0->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-editor>=1.0.4 (from inquirer<5.0,>=2.10.0->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting readchar>=3.0.6 (from inquirer<5.0,>=2.10.0->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading readchar-4.0.5-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.1->-r /content/efficientspeech/requirements.txt (line 1)) (2.1.2)\n",
            "Collecting pyjwt (from lightning-cloud>=0.5.34->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading PyJWT-2.7.0-py3-none-any.whl (22 kB)\n",
            "Collecting python-multipart (from lightning-cloud>=0.5.34->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from lightning-cloud>=0.5.34->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p-en->-r /content/efficientspeech/requirements.txt (line 11)) (2022.10.31)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->-r /content/efficientspeech/requirements.txt (line 18)) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->-r /content/efficientspeech/requirements.txt (line 18)) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa->-r /content/efficientspeech/requirements.txt (line 18)) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.15.2->-r /content/efficientspeech/requirements.txt (line 2)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.15.2->-r /content/efficientspeech/requirements.txt (line 2)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.15.2->-r /content/efficientspeech/requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (2.14.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->-r /content/efficientspeech/requirements.txt (line 18)) (3.1.0)\n",
            "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from starsessions<2.0,>=1.2.1->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (2.1.2)\n",
            "Collecting h11>=0.8 (from uvicorn<2.0->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->-r /content/efficientspeech/requirements.txt (line 14))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.1->-r /content/efficientspeech/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (0.2.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<15.0,>=12.3.0->lightning>=2.0.2->-r /content/efficientspeech/requirements.txt (line 3)) (0.1.2)\n",
            "Building wheels for collected packages: validators, tgt, pyworld, distance\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19579 sha256=f139cb3dc289c4d416befbf27a5a85b5a6a4e47d057406b2e8746a7a911bbdea\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/ed/dd/d3a556ad245ef9dc570c6bcd2f22886d17b0b408dd3bbb9ac3\n",
            "  Building wheel for tgt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tgt: filename=tgt-1.4.4-py3-none-any.whl size=28903 sha256=04954cbeecbd28f426f7605d7d962b3fafdcb13e1eb664b6b40452436444626b\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/e6/aa/821531faeb4e05a65d1c763570e90791467cf0c3f1622dc7e2\n",
            "  Building wheel for pyworld (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyworld: filename=pyworld-0.3.3-cp310-cp310-linux_x86_64.whl size=887692 sha256=b8df838f5b3bf2f527380d65fb71f1a4c2d2905571029ea76184dda2cf8ec5f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/50/a9/36b47c7f055bbee666a2b5718aaf85bce2152ef90f9bd10697\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16258 sha256=51b5ba7530aef8f3f2fcd5efbf123886efdb24d12002262fb2d14cce5194feeb\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n",
            "Successfully built validators tgt pyworld distance\n",
            "Installing collected packages: tgt, python-editor, distance, websockets, validators, unidecode, readchar, python-multipart, pyjwt, protobuf, ordered-set, numpy, multidict, lightning-utilities, humanfriendly, h11, frozenlist, einops, blessed, async-timeout, yarl, uvicorn, starlette, sounddevice, pyworld, onnx, inquirer, deepdiff, dateutils, croniter, coloredlogs, arrow, aiosignal, starsessions, onnxruntime, g2p-en, fastapi, aiohttp, lightning-cloud, torchmetrics, pytorch-lightning, lightning\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.8.4 aiosignal-1.3.1 arrow-1.2.3 async-timeout-4.0.2 blessed-1.20.0 coloredlogs-15.0.1 croniter-1.3.15 dateutils-0.6.12 deepdiff-6.3.0 distance-0.1.3 einops-0.6.1 fastapi-0.88.0 frozenlist-1.3.3 g2p-en-2.1.0 h11-0.14.0 humanfriendly-10.0 inquirer-3.1.3 lightning-2.0.2 lightning-cloud-0.5.36 lightning-utilities-0.8.0 multidict-6.0.4 numpy-1.23.5 onnx-1.14.0 onnxruntime-1.15.0 ordered-set-4.1.0 protobuf-3.20.2 pyjwt-2.7.0 python-editor-1.0.4 python-multipart-0.0.6 pytorch-lightning-2.0.2 pyworld-0.3.3 readchar-4.0.5 sounddevice-0.4.6 starlette-0.22.0 starsessions-1.3.0 tgt-1.4.4 torchmetrics-0.11.4 unidecode-1.3.6 uvicorn-0.22.0 validators-0.20.0 websockets-11.0.3 yarl-1.9.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# https://pytorch-lightning.readthedocs.io/en/1.2.10/advanced/tpu.html#tpu-terminology\n",
        "#!pip install cloud-tpu-client==0.10 # https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "#!pip install torch-xla --index-url https://pip.repos.neuron.amazonaws.com\n",
        "#!pip install wandb\n",
        "\n",
        "# Install requirements\n",
        "!pip install tensorboard\n",
        "!pip install -r /content/efficientspeech/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YAML helper functions"
      ],
      "metadata": {
        "id": "AH9otrsfpBdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "\n",
        "\n",
        "# YAML helper functions\n",
        "def get_yaml_path(name, config_dir):\n",
        "  return os.path.join(config_dir, name+'.yaml')\n",
        "\n",
        "\n",
        "def get_yaml_contents(name, config_dir):\n",
        "  with open(get_yaml_path(name, config_dir), 'r') as f:\n",
        "    return yaml.safe_load(f.read())\n",
        "            \n",
        "\n",
        "def write_yaml(name, contents, config_dir):\n",
        "  with open(get_yaml_path(name), 'w') as f:\n",
        "    f.write(yaml.dump(contents))"
      ],
      "metadata": {
        "id": "NiwMzcB9o8Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gsf9fkRz393z"
      },
      "source": [
        "Read existing preprocess.yaml and make a new preprocess.yaml configuration file with the paths we expect in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBTakj0RvjXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45b6174c-62df-4fdb-8ac6-03e6bf364778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading preprocess.yaml from \"/content/output_dataset/configs/MyDataset\"\n",
            "Writing preprocess.yaml in \"/content/efficientspeech/config/MyDataset/preprocess.yaml\":\n",
            "\n",
            "\n",
            "dataset: MyDataset\n",
            "path:\n",
            "  corpus_path: /content/output_dataset/corpus\n",
            "  lexicon_path: /content/efficientspeech/lexicon/librispeech-lexicon.txt\n",
            "  preprocessed_path: /content/output_dataset/preprocessed_data/MyDataset\n",
            "  raw_path: /content/output_dataset/raw_data\n",
            "preprocessing:\n",
            "  audio:\n",
            "    max_wav_value: 32768.0\n",
            "    sampling_rate: 22050\n",
            "  energy:\n",
            "    feature: phoneme_level\n",
            "    normalization: true\n",
            "  mel:\n",
            "    mel_fmax: 8000\n",
            "    mel_fmin: 0\n",
            "    n_mel_channels: 80\n",
            "  pitch:\n",
            "    feature: phoneme_level\n",
            "    normalization: true\n",
            "  stft:\n",
            "    filter_length: 1024\n",
            "    hop_length: 256\n",
            "    win_length: 1024\n",
            "  text:\n",
            "    language: en\n",
            "    max_length: 4096\n",
            "    text_cleaners:\n",
            "    - english_cleaners\n",
            "  val_size: 64\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import yaml\n",
        "try:\n",
        "    from yaml import CLoader as Loader, CDumper as Dumper\n",
        "except ImportError:\n",
        "    from yaml import Loader, Dumper\n",
        "\n",
        "\n",
        "existing_config_path = os.path.join(dataset_location, 'configs', dataset_name)\n",
        "dataset_config_dir = os.path.join(config_dir, dataset_name)\n",
        "\n",
        "# preprocess.yaml - update paths and add field to text\n",
        "print(f'Reading preprocess.yaml from \"{existing_config_path}\"')\n",
        "pp = get_yaml_contents('preprocess', existing_config_path)\n",
        "pp['dataset'] = dataset_name\n",
        "pp['path']['corpus_path'] = f\"{dataset_location}/corpus\"\n",
        "pp['path']['lexicon_path'] = lexicon_path\n",
        "pp['path']['raw_path'] = f\"{dataset_location}/raw_data\"\n",
        "pp['path']['preprocessed_path'] = f\"{dataset_location}/preprocessed_data/{dataset_name}\"\n",
        "\n",
        "print(f'Writing preprocess.yaml in \"{dataset_config_dir}/preprocess.yaml\":')\n",
        "\n",
        "with open(os.path.join(dataset_config_dir, 'preprocess.yaml'), 'w') as f:\n",
        "  f.write(yaml.dump(pp))\n",
        "print('\\n')\n",
        "\n",
        "!cat $dataset_config_dir\\/preprocess.yaml\n",
        "\n",
        "\n",
        "# pp_config = f\"\"\"dataset: \"{dataset_name}\"\n",
        "\n",
        "# path:\n",
        "#   corpus_path: \"{dataset_location}/corpus\"\n",
        "#   lexicon_path: \"{lexicon_path}\"\n",
        "#   raw_path: \"{dataset_location}/raw_data\"\n",
        "#   preprocessed_path: \"{dataset_location}/preprocessed_data\"\n",
        "\n",
        "# preprocessing:\n",
        "#   val_size: 64\n",
        "#   text:\n",
        "#     text_cleaners: [\"english_cleaners\"]\n",
        "#     language: \"en\"\n",
        "#     max_length: 4096\n",
        "#   audio:\n",
        "#     sampling_rate: 22050\n",
        "#     max_wav_value: 32768.0\n",
        "#   stft:\n",
        "#     filter_length: 1024\n",
        "#     hop_length: 256\n",
        "#     win_length: 1024\n",
        "#   mel:\n",
        "#     n_mel_channels: 80\n",
        "#     mel_fmin: 0\n",
        "#     mel_fmax: 8000 # please set to 8000 for HiFi-GAN vocoder, set to null for MelGAN vocoder\n",
        "#   pitch:\n",
        "#     feature: \"phoneme_level\" # support 'phoneme_level' or 'frame_level'\n",
        "#     normalization: True\n",
        "#   energy:\n",
        "#     feature: \"phoneme_level\" # support 'phoneme_level' or 'frame_level'\n",
        "#     normalization: True\n",
        "# \"\"\"\n",
        "\n",
        "# # Write config to file\n",
        "# with open(f'{dataset_config_dir}/preprocess.yaml', mode='w') as f:\n",
        "#   f.write(pp_config.expandtabs())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#  \n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6F2fWSBITMTk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq882mrs2E42"
      },
      "source": [
        "# Train a checkpoint \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Launch TensorBoard"
      ],
      "metadata": {
        "id": "LaDwDvFNkJ3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir /content/efficientspeech/lightning_logs/"
      ],
      "metadata": {
        "id": "_rglH-KDOr5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run training"
      ],
      "metadata": {
        "id": "h0-1pUyp6c01"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-fMDTjFHF_O",
        "outputId": "1779fc85-ad0e-4d1f-b9fe-6e095db9ee75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running training with arguments: --preprocess-config /content/efficientspeech/config/MyDataset/preprocess.yaml  --accelerator gpu --devices 1 --num_workers 4 --precision 16-mixed --batch-size 128 --head 1 --reduction 4 --expansion 1 --kernel-size 3 --n-blocks 2 --block-depth 2 --max_epochs 5000 --infer-device cuda\n",
            "/content/efficientspeech\n",
            "Removing weight norm...\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name        | Type        | Params\n",
            "--------------------------------------------\n",
            "0 | phoneme2mel | Phoneme2Mel | 266 K \n",
            "1 | hifigan     | Generator   | 925 K \n",
            "--------------------------------------------\n",
            "266 K     Trainable params\n",
            "926 K     Non-trainable params\n",
            "1.2 M     Total params\n",
            "4.770     Total estimated model params size (MB)\n",
            "2023-06-01 02:53:51.811205: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-01 02:53:52.687426: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Sanity Checking: 0it [00:00, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "  rank_zero_warn(\n",
            "Epoch 9: 100% 3/3 [00:01<00:00,  2.78it/s, v_num=3, mel=5.170, pitch=2.030, energy=1.030, dur=3.110, loss=60.90, lr=0.00018]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: 100% 3/3 [00:01<00:00,  1.51it/s, v_num=3, mel=5.170, pitch=2.030, energy=1.030, dur=3.110, loss=60.90, lr=0.00018]\n",
            "Epoch 19: 100% 3/3 [00:01<00:00,  1.88it/s, v_num=3, mel=3.950, pitch=1.860, energy=0.755, dur=1.790, loss=46.50, lr=0.00038]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19: 100% 3/3 [00:02<00:00,  1.21it/s, v_num=3, mel=3.950, pitch=1.860, energy=0.755, dur=1.790, loss=46.50, lr=0.00038]\n",
            "Epoch 29: 100% 3/3 [00:01<00:00,  1.78it/s, v_num=3, mel=2.160, pitch=1.700, energy=0.576, dur=0.844, loss=27.00, lr=0.00058]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29: 100% 3/3 [00:02<00:00,  1.07it/s, v_num=3, mel=2.160, pitch=1.700, energy=0.576, dur=0.844, loss=27.00, lr=0.00058]\n",
            "Epoch 39: 100% 3/3 [00:01<00:00,  2.77it/s, v_num=3, mel=1.300, pitch=1.490, energy=0.548, dur=0.383, loss=17.50, lr=0.00078]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39: 100% 3/3 [00:02<00:00,  1.46it/s, v_num=3, mel=1.300, pitch=1.490, energy=0.548, dur=0.383, loss=17.50, lr=0.00078]\n",
            "Epoch 49: 100% 3/3 [00:01<00:00,  2.76it/s, v_num=3, mel=1.230, pitch=1.360, energy=0.489, dur=0.279, loss=16.30, lr=0.00098]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49: 100% 3/3 [00:02<00:00,  1.36it/s, v_num=3, mel=1.230, pitch=1.360, energy=0.489, dur=0.279, loss=16.30, lr=0.00098]\n",
            "Epoch 59: 100% 3/3 [00:01<00:00,  2.74it/s, v_num=3, mel=1.130, pitch=1.110, energy=0.457, dur=0.273, loss=14.70, lr=0.001]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 59: 100% 3/3 [00:02<00:00,  1.37it/s, v_num=3, mel=1.130, pitch=1.110, energy=0.457, dur=0.273, loss=14.70, lr=0.001]\n",
            "Epoch 69: 100% 3/3 [00:01<00:00,  2.84it/s, v_num=3, mel=1.070, pitch=0.903, energy=0.436, dur=0.265, loss=13.60, lr=0.001]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 69: 100% 3/3 [00:02<00:00,  1.41it/s, v_num=3, mel=1.070, pitch=0.903, energy=0.436, dur=0.265, loss=13.60, lr=0.001]\n",
            "Epoch 79: 100% 3/3 [00:01<00:00,  1.90it/s, v_num=3, mel=1.060, pitch=0.708, energy=0.413, dur=0.270, loss=13.10, lr=0.001]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 79: 100% 3/3 [00:02<00:00,  1.06it/s, v_num=3, mel=1.060, pitch=0.708, energy=0.413, dur=0.270, loss=13.10, lr=0.001]\n",
            "Epoch 89: 100% 3/3 [00:01<00:00,  2.58it/s, v_num=3, mel=1.010, pitch=0.635, energy=0.386, dur=0.268, loss=12.40, lr=0.001]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 89: 100% 3/3 [00:02<00:00,  1.21it/s, v_num=3, mel=1.010, pitch=0.635, energy=0.386, dur=0.268, loss=12.40, lr=0.001]\n",
            "Epoch 99: 100% 3/3 [00:01<00:00,  2.82it/s, v_num=3, mel=0.978, pitch=0.474, energy=0.340, dur=0.261, loss=11.70, lr=0.001]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
            "Training time: 0:02:17.415095\n",
            "Epoch 99: 100%|██████████| 3/3 [00:02<00:00,  1.25it/s, v_num=3, mel=0.978, pitch=0.474, energy=0.340, dur=0.261, loss=11.70, lr=0.001]\n",
            "\n",
            "                                                  \u001b[A^C\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Train\n",
        "pp_config_path = os.path.join(dataset_config_dir, 'preprocess.yaml')\n",
        "pp_config_arg = f'--preprocess-config {pp_config_path}'\n",
        "training_opts = ' '.join([pp_config_arg, cmd_line_opts])\n",
        "\n",
        "print(f'Running training with arguments: {training_opts}')\n",
        "\n",
        "%cd /content/efficientspeech/\n",
        "!python /content/efficientspeech/train.py $training_opts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run inference on latest trained checkpoint"
      ],
      "metadata": {
        "id": "JayYMPZAHQa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio, display\n",
        "import os\n",
        "\n",
        "sentence = 'The quick brown fox jumped over the lazy dog.' #@param {type:'string'}\n",
        "\n",
        "%cd /content/efficientspeech/\n",
        "\n",
        "# Get latest run checkpoint\n",
        "latest_run_folder = !ls -td -- lightning_logs/* | head -n 1\n",
        "latest_run_folder = latest_run_folder[0]\n",
        "latest_run_name = os.path.basename(latest_run_folder)\n",
        "ckpt_folder = os.path.join(latest_run_folder, 'checkpoints')\n",
        "latest_ckpt = !ls -td -- $ckpt_folder\\/* | head -n 1\n",
        "latest_ckpt = os.path.abspath(latest_ckpt[0])\n",
        "latest_ckpt_name = os.path.basename(latest_ckpt)\n",
        "# Output wav \n",
        "output_wav_name = latest_run_name + '.wav'\n",
        "\n",
        "print(f'Found checkpoint \"{latest_ckpt}')\n",
        "\n",
        "# Run inference with latest checkpoint\n",
        "inference_args = f'--checkpoint {latest_ckpt} {model_opts} ' \\\n",
        "  f'--infer-device {infer_device} --text \"{sentence}\" ' \\\n",
        "  f'--wav-filename {output_wav_name}'\n",
        "print(f'Running inference with arguments: {inference_args}')\n",
        "!python demo.py $inference_args\n",
        "\n",
        "# Display inference result\n",
        "output_wav_path = os.path.join('/content/efficientspeech/outputs', output_wav_name)\n",
        "print(f'\\nInference result: {output_wav_path}')\n",
        "display(Audio(os.path.abspath(output_wav_path)))"
      ],
      "metadata": {
        "id": "aAspCqnQHP5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#  \n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IKyrgo9wX8Rg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbJYqQOhOexo"
      },
      "source": [
        "# *VERY IMPORTANT* - Copy all checkpoints to your drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4X6g9ONjOeKO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b89c6eb8-8c6a-47a9-fe3a-287770cca800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying checkpoints to \"/content/drive/MyDrive/saved_checkpoints/MyDataset\"\n",
            "'/content/efficientspeech/lightning_logs/version_0/events.out.tfevents.1685587239.44c9d25ae1f2.2131.0' -> '/content/drive/MyDrive/saved_checkpoints/MyDataset/version_0/events.out.tfevents.1685587239.44c9d25ae1f2.2131.0'\n",
            "'/content/efficientspeech/lightning_logs/version_0/hparams.yaml' -> '/content/drive/MyDrive/saved_checkpoints/MyDataset/version_0/hparams.yaml'\n",
            "'/content/efficientspeech/lightning_logs/version_1/events.out.tfevents.1685587278.44c9d25ae1f2.2502.0' -> '/content/drive/MyDrive/saved_checkpoints/MyDataset/version_1/events.out.tfevents.1685587278.44c9d25ae1f2.2502.0'\n",
            "'/content/efficientspeech/lightning_logs/version_1/hparams.yaml' -> '/content/drive/MyDrive/saved_checkpoints/MyDataset/version_1/hparams.yaml'\n",
            "'/content/efficientspeech/lightning_logs/version_2/events.out.tfevents.1685587993.44c9d25ae1f2.10103.0' -> '/content/drive/MyDrive/saved_checkpoints/MyDataset/version_2/events.out.tfevents.1685587993.44c9d25ae1f2.10103.0'\n",
            "'/content/efficientspeech/lightning_logs/version_2/hparams.yaml' -> '/content/drive/MyDrive/saved_checkpoints/MyDataset/version_2/hparams.yaml'\n",
            "'/content/efficientspeech/lightning_logs/version_3/events.out.tfevents.1685588034.44c9d25ae1f2.10520.0' -> '/content/drive/MyDrive/saved_checkpoints/MyDataset/version_3/events.out.tfevents.1685588034.44c9d25ae1f2.10520.0'\n",
            "'/content/efficientspeech/lightning_logs/version_3/hparams.yaml' -> '/content/drive/MyDrive/saved_checkpoints/MyDataset/version_3/hparams.yaml'\n",
            "'/content/efficientspeech/lightning_logs/version_3/checkpoints/epoch=89-step=270.ckpt' -> '/content/drive/MyDrive/saved_checkpoints/MyDataset/version_3/checkpoints/epoch=89-step=270.ckpt'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "output_folder_path = os.path.join(output_dir, dataset_name)\n",
        "print(f'Copying checkpoints to \"{output_folder_path}\"')\n",
        "\n",
        "# Create the target directory if it doesn't exist\n",
        "if not os.path.exists(output_folder_path):\n",
        "   os.makedirs(output_folder_path)\n",
        "   print(f'Created folder \"{output_folder_path}\"')\n",
        "\n",
        "# Iterate through all subdirectories in the log directory\n",
        "source_dir = '/content/efficientspeech/lightning_logs'\n",
        "!cp -v -r $source_dir\\/* $output_folder_path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#  \n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dnmMHHP-XzMb"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}